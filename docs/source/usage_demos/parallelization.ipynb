{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multithreaded computation with shared memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two types of parallel dispatch\n",
    "\n",
    "An easy job to parallelize is doing a simple thing to a lot of data. A common example here would be filtering a multi-channel signal.\n",
    "\n",
    "<img src=\"images/vectorized_task.png\" width=\"700pt\">\n",
    "\n",
    "This job can be split by splitting (muxing/demuxing) the dataset:\n",
    "\n",
    "<img src=\"images/vectorized_task_para.png\" width=\"700pt\">\n",
    "\n",
    "Splitting the large data array is a special case of the more general pattern of doing an arbitrary job for multiple input cases. An example for this case might be a grid-search to optimize hyperparameters in a machine learning design.\n",
    "\n",
    "<img src=\"images/dispatched_jobs.png\" width=\"700pt\">\n",
    "\n",
    "The general job dispatch pattern is covered in the JobRunner-demo notebook. This notebook will work through two examples of array splitting, and then look at the multiprocessing details behind parallelization.\n",
    "\n",
    "### Contents\n",
    "\n",
    "* Array splitting\n",
    "  * Splitting existing methods\n",
    "  * Applying splitting to new code\n",
    "* Process start modes in Python multiprocessing\n",
    "  + contextualized processesing, shared memory, etc\n",
    "* Using shared memory\n",
    "  + duality of numpy.ndarray and sharedctypes.Array\n",
    "  + identical API for spawn & fork\n",
    "* Logging contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Array splitting\n",
    "\n",
    "To demonstrate splitting a large array into smaller batches, we'll look at a good old, single-threaded Gaussian kernel filtering routine. This is effectively a finite impulse response (FIR) filter, where the complexity grows with the Gauss function width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "from functools import wraps\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal import lfilter\n",
    "\n",
    "from ecogdata.parallel.mproc import parallel_context, make_stderr_logger\n",
    "from ecogdata.parallel.array_split import split_at, split_output, split_optional_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use fork context if possible -- read below for details.\n",
    "\n",
    "try:\n",
    "    parallel_context.ctx = 'fork'\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a 100x Gaussian kernel FIRs with an order of 320 (+/- 4 sigma width, sigma=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_freq_signal = parallel_context.shared_copy(np.random.randn(100, 400000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline single-threaded timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timed_run(f, msg='runtime'):\n",
    "    @wraps(f)\n",
    "    def runner(*args, **kwargs):\n",
    "        tic = time()\n",
    "        r = f(*args, **kwargs)\n",
    "        toc = time()\n",
    "        print('{} {} sec'.format(msg, toc - tic))\n",
    "        return r\n",
    "    return runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 40\n",
    "low_freq_signal = timed_run(gaussian_filter1d)(high_freq_signal, sigma, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic array splitting using the ``ecogdata.parallel.array_split.split_at`` \"decorator\"\n",
    "\n",
    "A Python decorator is a function whose input and output are functions. It basically modifies the behavior of the original function. The function ``timed_run`` defined above is a type of decorator.\n",
    "\n",
    "For array splitting, the decorator created within ``split_at`` drives the original function in subprocesses. Each subprocess acts on a separate batch of the full array(s). Decorators can be called with normal syntax, or using special syntax in combination with ``def`` \n",
    "\n",
    "```python\n",
    "@split_at()\n",
    "def normal_function(x):\n",
    "    return 2 * x\n",
    "```\n",
    "\n",
    "This would create function that looks like ``normal_function``, but operates in subprocesses.\n",
    "\n",
    "To parallelize an existing function, use normal syntax. The arguments say to split the array in the first argument position and to splice the output array in the first output position. Do this to see *all* possible configurations\n",
    "\n",
    "```python\n",
    "help(split_at)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing this wrapping in two steps for clarity\n",
    "\n",
    "# 1) parameterize the array splitting decorator\n",
    "splitter = split_at(split_arg=(0,), splice_at=(0,))\n",
    "# 2) transform the basic function using the decorator\n",
    "gaussian_filter1d_para = splitter(gaussian_filter1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may or may not show a speedup, depending on what process start method is active. More on that next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 40\n",
    "low_freq_signal_2 = timed_run(gaussian_filter1d_para)(high_freq_signal, sigma, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test consistancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(low_freq_signal_2 == low_freq_signal).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smarter shared output splitting for simple input-output functions\n",
    "\n",
    "The speedup is modest. Read the following sections for more details here. Briefly, the major *extra* costs of subprocess computation are\n",
    "\n",
    "1. launching processes\n",
    "1. sharing memory to processes\n",
    "1. gathering results from processes\n",
    "\n",
    "We only really have tools to improve (2) and (3). For getting memory to processes, we're already in good shape using shared memory:\n",
    "\n",
    "```python\n",
    "high_freq_signal = parallel_context.shared_copy(np.random.randn(100, 400000))\n",
    "```\n",
    "\n",
    "The parallel driver doesn't know anything about handling the output. So for the gathering step, the subprocesses actually have to \"pickle\" (serialize into a byte-stream) output arrays to be reconstituted in the main process. Better if we can put the output into shared memory that is already available to the main process.\n",
    "\n",
    "It would be a two (or three) step process to modify the existing filter function\n",
    "\n",
    "1. Define an outer function that gobbles the output into shared memory, rather than returning it\n",
    "\n",
    "```python\n",
    "def gaussian_filter1d_void(x, y, **kwargs):\n",
    "    \"\"\"Turn gaussian_filter1d into traditional C \"void\" behavior\"\"\"\n",
    "    y[:] = gaussian_filter1d(x, **kwargs)\n",
    "```\n",
    "\n",
    "2. Then parallel-wrap the *void* function, with splitting on the input & output arguments\n",
    "\n",
    "```python\n",
    "gaussian_filter1d_para = split_at(split_arg=(0, 1))(gaussian_filter1d_void)\n",
    "```\n",
    "\n",
    "This function can be used with predefined shared output. Optional third step\n",
    "\n",
    "3. Define convenience function to allocate new memory if needed\n",
    "\n",
    "```python\n",
    "def gaussian_filter1d_wrap(x, out=None, inplace=False, **kwargs):\n",
    "    if out is None:\n",
    "        if inplace:\n",
    "            out = x\n",
    "        else:\n",
    "            out = parallel_context.shared_ndarray(x.shape, typecode=x.dtype.char)\n",
    "    gaussian_filter1d_para(x, out, **kwargs)\n",
    "    return out\n",
    "```\n",
    "\n",
    "### Special-purpose splitters for shared output\n",
    "\n",
    "Two narrow purpose variations of ``split_at`` are defined for the fairly common scenario where the output is the same size as the input, and can be split into similiar subsets. These functions essentially do the three modifications above as part of the \"decoration\" process.\n",
    "\n",
    "* ``split_output``: can be used for general functions that return an output the same size as the input\n",
    "  + adds 'out' and 'inplace' keyword arguments to the wrapped method, like above\n",
    "* ``split_optional_output``: used in the case where the original function has an option to put the results in a pre-defined array\n",
    "  + also adds the 'inplace' option to the wrapped method\n",
    "\n",
    "The first variation can (probably) be used in all such cases. However, it is probably most efficient to use the orignal function's output option if it's there.\n",
    "\n",
    "In fact, gaussian_filter1d has an \"output\" option, so we'll use the second variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing this wrapping in two steps for clarity\n",
    "\n",
    "splitter = split_optional_output(output_kwarg='output', split_arg=(0,))\n",
    "gaussian_filter1d_para = splitter(gaussian_filter1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can work three ways: \n",
    "\n",
    "1. the method creates new memory (default)\n",
    "2. we define memory ahead of time (matching the single-thread behavior)\n",
    "3. we do inplace (putting the output into the input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 40\n",
    "low_freq_signal_2 = timed_run(gaussian_filter1d_para, msg='Default runtime')(high_freq_signal, sigma, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(low_freq_signal_2 == low_freq_signal).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = parallel_context.shared_ndarray(high_freq_signal.shape, typecode='d')\n",
    "sigma = 40\n",
    "low_freq_signal_2 = timed_run(gaussian_filter1d_para, \n",
    "                              msg='Predef memory runtime')(high_freq_signal, sigma, output=output, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(low_freq_signal_2 is output), (low_freq_signal_2 == low_freq_signal).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 40\n",
    "low_freq_signal_2 = timed_run(gaussian_filter1d_para,\n",
    "                              msg='Inplace memory runtime')(high_freq_signal, sigma, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(low_freq_signal_2 is high_freq_signal), (low_freq_signal_2 == low_freq_signal).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying splitters for novel code\n",
    "\n",
    "This section applies some new tricks\n",
    "\n",
    "* decorating a new function definition\n",
    "* splitting general multidimensional arrays\n",
    "* logging\n",
    "* parallel state toggling\n",
    "\n",
    "For spawned processes, parallelized code needs to be defined elsewhere and imported into the script. *The hack here is to write the same code definition to a temporary module. This is only for demonstration, and not normal practice.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ecogdata.util import input_as_2d\n",
    "from ecogdata.parallel.mproc import make_stderr_logger, parallel_controller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decorate this function two ways. From inner to outer (order is important):\n",
    "\n",
    "1. split with automatic output shared memory\n",
    "2. reshape input/output arrays to be 2D (*this should be applied after splitting, since arrays are split on the first dimension*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if parallel_context.context_name == 'fork':\n",
    "    @input_as_2d(in_arr=0, out_arr=0)\n",
    "    @split_output(n_jobs=14)\n",
    "    def convolve(x, kernel):\n",
    "        y = np.empty_like(x)\n",
    "        p = len(kernel)\n",
    "        logger = parallel_context.get_logger()\n",
    "        logger.info('{}'.format(x.shape))\n",
    "        m, n = x.shape\n",
    "        for i in range(p - 1, n):\n",
    "            y[:, i] = x[:, i] * kernel[p - 1]\n",
    "            y[:, i] = np.sum(x[:, i - (p - 1):i + 1] * kernel[::-1], axis=1)\n",
    "        return y\n",
    "else:\n",
    "    code_def = \"\"\"\n",
    "import numpy as np\n",
    "from ecogdata.util import input_as_2d\n",
    "from ecogdata.parallel.array_split import split_output\n",
    "from ecogdata.parallel.mproc import parallel_context\n",
    "\n",
    "\n",
    "@input_as_2d(in_arr=0, out_arr=0)\n",
    "@split_output(n_jobs=14)\n",
    "def convolve(x, kernel):\n",
    "    y = np.empty_like(x)\n",
    "    p = len(kernel)\n",
    "    logger = parallel_context.get_logger()\n",
    "    logger.info('{}'.format(x.shape))\n",
    "    m, n = x.shape\n",
    "    for i in range(p - 1, n):\n",
    "        y[:, i] = x[:, i] * kernel[p - 1]\n",
    "        y[:, i] = np.sum(x[:, i - (p - 1):i + 1] * kernel[::-1], axis=1)\n",
    "    return y\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        with open('temp_module.py', 'w') as f:\n",
    "            f.write(code_def)\n",
    "        from temp_module import convolve\n",
    "    except:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general multidimensional ndarray to split\n",
    "\n",
    "x = parallel_context.shared_copy(np.random.randn(10, 20, 20000))\n",
    "kernel = np.ones(1500) / 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logging in the new function writes to the info channel. Under the error channel, nothing will show up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with make_stderr_logger('error'):\n",
    "    y = timed_run(convolve)(x, kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the info channel, we'll see the messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are no subprocess messages in spawn?\n",
    "    \n",
    "with make_stderr_logger('info'):\n",
    "    y = convolve(x, kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If parallel should be shut off, use the parallel controller. This can be set in a sticky way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_controller.state = False\n",
    "print('Parallel state is now:', bool(parallel_controller))\n",
    "parallel_controller.state = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or better, in a way that only affect the context. Use it here to compare the parallel / serial timing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with parallel_controller(False):\n",
    "    y = timed_run(convolve)(x, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if parallel_context.context_name != 'fork':\n",
    "    import os\n",
    "    if os.path.exists('temp_module.py'):\n",
    "        os.unlink('temp_module.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiprocessing modes in Python\n",
    "\n",
    "Parallel processing is supported in Python by way of a multiprocessing framework, where distinct processes are generated to run simultaneously. There are two primary modes that Python supports for creating subprocesses: \"spawn\" and \"fork\". (Another mode, \"forkserver\", is similar to spawn but not discussed here.) Mac and Linux support all subprocess types. Windows only support spawning. \n",
    "\n",
    "The tools discussed here are meant to operate consistently with any subprocess context. However, there are generally some limitations that will arise under spawning. These quirks (which I frankly don't understand too well) are related to saving and restoring the process state through \"pickling\". One major limitation with spawn affects interactive sessions (e.g. Notebooks). Generally, the code to parallelize should be defined elsewhere and *imported* to the session--that is all supported by pickling. Fork does not have that limitation, making interactive work a bit easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good practice for scripting\n",
    "\n",
    "Parallelizing code that is *itself* multithreaded can lead to a resource race called \"oversubscription,\" where each subprocess tries to launch into a routine that will consume multiple CPU cores or threads. This typically occurs in system libraries for linear algebra, fft, and certain other cases. You can set the runtime mode of certain libraries before linking to them, like this (not all variables apply to all systems, but it doesn't hurt to set them).\n",
    "\n",
    "```python\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\" # equivalent to shell: export OMP_NUM_THREADS=1\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\" # export OPENBLAS_NUM_THREADS=1\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\" # export MKL_NUM_THREADS=1\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\" # export VECLIB_MAXIMUM_THREADS=1\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\" # export NUMEXPR_NUM_THREADS=1\n",
    "```\n",
    "    \n",
    "## Run-time context management\n",
    "\n",
    "While changing the default multiprocessing context is not usually, these use cases apply:\n",
    "\n",
    "* choosing \"fork\" for interactive work, if available\n",
    "* testing parallel code under multiple contexts\n",
    "\n",
    "``ecogdata`` and similar libraries take advantage of a stateful parallel context that can be switched during runtime. The state object (``ecogdata.parallel.mproc.parallel_context``) has a name space that depends on what the present start mode is. In addition to the standard Python multiprocessing namespace, ``ecogdata.parallel`` adds a few conxtent-dependent shared memory tools as well (more below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_context.ctx = 'fork'\n",
    "print('Parallel mode:', parallel_context.context_name)\n",
    "print('Mode objects:', parallel_context.Process, parallel_context.SharedmemManager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The context can be changed either by reassignment, or in a contextualized block (using ``with`` syntax)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with parallel_context.switch_context('spawn'):\n",
    "    print('Parallel mode:', parallel_context.context_name)\n",
    "    print('Mode objects:', parallel_context.Process, parallel_context.SharedmemManager)\n",
    "# after leaving the context, parallel is switched to the previous state\n",
    "print('Parallel mode:', parallel_context.context_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared memory\n",
    "\n",
    "A typical parallel pattern is to split a large amount of data into smaller batches. By default, data objects that are passed to a subprocess need to be \"pickled\", or serialized into a string of bytes, and then reconstructed in subprocess memory. This is usually true for forking and always true for spawning. Splitting a large array via serialization can be very time consuming, and duplicates memory consumption (*footnote for copy-on-write in forked processes).\n",
    "\n",
    "Shared memory is a mechanism for providing a single data object that is accessible to the parent process and subprocesses. In particular, a numpy ndarray can have an underlying shared memory buffer, making it acessible by all processes. The module ``ecogdata.parallel.sharemem`` implements a ``SharedmemManager`` class that manages two faces of a common shared memory buffer:\n",
    "\n",
    "* SharedmemManager.shm: ``multiprocessing.sharedctypes.Array``\n",
    "* SharedmemManager.tonumpyarray: presents as a ``numpy.ndarray``\n",
    "\n",
    "\n",
    "Creating shared memory is also handled through the manager via ``shared_ndarray`` \n",
    "\n",
    "```python\n",
    "@classmethod\n",
    "def shared_ndarray(cls, shape, typecode='d'):\n",
    "    ...\n",
    "```\n",
    "\n",
    "and ``shared_copy``\n",
    "\n",
    "\n",
    "```python\n",
    "@classmethod\n",
    "def shared_copy(cls, x):\n",
    "    ...\n",
    "```\n",
    "\n",
    "### When to create shared memory\n",
    "\n",
    "Complicating matters, the interaction of ndarrays and shared memory differs substantially in forked and spawned subprocesses. When spawning, all ``SharememManager`` objects point to actual shared memory, copying existing data if necessary. **With spawn, arrays that will be used with parallelized methods should be allocated as shared memory whenever possible.**\n",
    "\n",
    "Under fork, there is more flexibility, and using shared memory requires some planning about whether ndarrays are created as shared memory or regular memory.  \n",
    "\n",
    "**Subprocesses only do read-only access to arrays**\n",
    "\n",
    "If a forked process is only *reading* from data arrays, it can use a memory pointer that looks like shared memory, even though it isn't. In this case, you don't need to create new shared memory. Shared memory managers can be created with *existing* ndarrays (takes advantage of the \"copy-on-write\" feature of forked process memory).\n",
    "\n",
    "In the spawning context, creating a ``SharedmemManager`` for an existing array in plain memory will create a shared memory copy of that array. If that is a large array, you may want to allocate as shared memory to begin with.\n",
    "\n",
    "**Subprocesses need read & write access to arrays**\n",
    "\n",
    "Shared memory must be allocated. If \"fake\" shared memory is used, all write-backs to the array by the subprocess will be lost to the parent process.\n",
    "\n",
    "**Subprocesses return large amounts of data**\n",
    "\n",
    "Regardless of the read/write question, shared memory should be allocated to store the output data. This will avoid the time cost for serializing outputs back to the parent process.\n",
    "\n",
    "\n",
    "### Memory management in basic Python \n",
    "In the future, the shared memory tools here may be replaced by the new SharedMemoryManager in Python. For the time being, this feature may be buggy (https://bugs.python.org/issue38119)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
