{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "from time import time\n",
    "from ecogdata.expconfig import session_info, load_params, OVERRIDE\n",
    "from ecogdata.devices.data_util import params_table\n",
    "from ecogdata.devices.load.active_electrodes import ActiveLoader\n",
    "from ecogdata.datasource.memmap import MemoryBlowOutError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "==================================\n",
    "MappedSource and array abstrations\n",
    "==================================\n",
    "\n",
    "Much effort has been put towards enabling consistent ecog signal interaction for file-mapped (\"mapped\") memory-loaded (\"loaded\") sources. This notebook demonstrates the workings of :py:class:`ecogdata.datasource.memmap.MappedSource` and data tyeps in :py:mod:`ecogdata.datasource.array_abstractions`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study: active electrode mapping\n",
    "\n",
    "To motivate the different requirements of mapping logic, we'll look at a recording from the SiO2 rat active electrode (Chiang, STM 2019). These recordings were made with National Instruments and saved to TDMS file format, which are either converted to HDF5 via Python or via Matlab scripts. Both variations may be found, and they are basically interchangable modulo a transpose in the \"data\" array.\n",
    "\n",
    "**Get the session info for a recording from an implanted array.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = session_info('16011/2016-09-01_active', params_table=params_table)\n",
    "daq = info.daq\n",
    "headstage = info.headstage\n",
    "electrode = info.electrode\n",
    "bnc = info.bnc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A transposed file to map\n",
    "# f_path = '/Users/mike/experiment_data/Viventi 2016-09-01 Active Rat 16011/test_001.mat'\n",
    "f_path = os.path.join(info.nwk_path, 'test_001.mat')\n",
    "with h5py.File(f_path, 'r') as f:\n",
    "    print('data shape:', f['data'].shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "The :py:class:`ActiveLoader <ecogdata.devices.load.active_arrays.ActiveLoader>` class knows to look for either .mat or .h5 files, and then how to map the correct electrode data and trigger channels from the files. In addition, these recordings had extra channels including the leakage current."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = ActiveLoader(info.nwk_path, 'test_001', electrode, daq, headstage, bnc=bnc, trigger_idx=0, mapped=True)\n",
    "channel_map, electrode_channels, other_channels, ref_channels = loader.make_channel_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "64 electrode data channels were stored within the first 80 channels, *but not contiguously*. (Those missing channels  included the leakage measurements.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(electrode_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "Each group of 8 channels corresponded to the same MUX, and were physically on the same electrode array column. But to make the mapping maximally complicated\n",
    "\n",
    "* physical rows are permuted within each MUX!\n",
    "* physical columns are permuted across MUXes!\n",
    "\n",
    "Note the row/column sequence of the first two MUXes (16 channels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the ecog array (row, col) of each channel\n",
    "print(list(zip(*channel_map.to_mat()))[:16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logic of MappedSource\n",
    "\n",
    "The requirements for providing consistent array interaction with file-mapping electrode data are these:\n",
    "\n",
    "* Expose a (mutable) subset of on-disk data that correspond to the ecog signal array\n",
    "  + This set needs to be mutable to enable channel selection\n",
    "* Expose slicing syntax: `output_array = a[1:3, 20:40]` and `a[1:3, 20:40] = input_array`\n",
    "* Guard against loading too much data at a time, since the raw data might be massive\n",
    "* Present signal matrix orientation consistently, irrespective of raw data tranposes\n",
    "* Map multiple files as if they were joined end on end (*only if every file has the same layout*)\n",
    "\n",
    "For a guide to generic DataSource interaction, see the [data sources notebook](data_sources.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the source data file and create a dataset \"bunch\"\n",
    "dataset = loader.create_dataset()\n",
    "print(dataset)\n",
    "mapped_source = dataset.data\n",
    "print('Electrode signal shape:', mapped_source.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Electrode channels & active channel selection\n",
    "\n",
    "The datasource for this dataset is a `MappedSource`. Note that its shape leads with 64 channels. This is the convention for all `ElectrodeDataSource` types.\n",
    "\n",
    "The set of electrode channels informs the MappedSource which channels from the underlying \"data_buffer\" to expose. These are the channels we saw before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MappedSource channels:', mapped_source._electrode_channels)\n",
    "print()\n",
    "print('Full data buffer shape:', mapped_source.data_buffer.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Note**: Attributes leading with an underscore (e.g. \"_.electrode_channels\") are considered to be \"private\". There is no notion of private and public class members in Python, so this is only understood by convention. The idea is that these attributes or methods should not be used/abused unless you are quite certain about what you are doing.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mapped source also has active channels, which are a subset of the electrode channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Active subset:', mapped_source._active_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being a \"private\" attribute, this set should not be manipulated directly. Instead, you can apply a channel mask in the form of a 64-channel binary array (the value of \"False\" means de-select a channel). Suppose there was a switching problem on the first (in channel-space) entire electrode column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get current mask: all True by default\n",
    "mask = mapped_source.binary_channel_mask\n",
    "mask[:8] = False\n",
    "mapped_source.set_channel_mask(mask)\n",
    "print('New active subset:', mapped_source._active_channels)\n",
    "print()\n",
    "print('New shape:', mapped_source.shape)\n",
    "print()\n",
    "print('Buffer shape:', mapped_source.data_buffer.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the active channels now exclude the de-selected channels. The data buffer, of course, is not changed.\n",
    "\n",
    "In practice, the ChannelMap should also be synchronized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_map = dataset.chan_map.subset(mask)\n",
    "sub_map.image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The channel mask can also be reset or unset on the same mapped source. For example, to unset just provide an all-True mask (or provide the value `None`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[:] = True\n",
    "mapped_source.set_channel_mask(mask)\n",
    "# This also works to reset:\n",
    "mapped_source.set_channel_mask(None)\n",
    "print('Shape:', mapped_source.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array access\n",
    "\n",
    "A `MappedSource` exposes read/write slicing with syntax that's (mostly) compatible with regular `numpy.ndarray` types. This access follows from a stack of logic abstractions, from bottom to top:\n",
    "\n",
    "* `h5py.Dataset`: \n",
    "  + exposes slicing syntax on the underlying HDF5 file, mapping slice ranges to the correct blocks in the dataset B-tree.\n",
    "* `HDF5Buffer` and `BufferBinder` types in `ecogdata.datasource.array_abstractions`: \n",
    "  + scatter-gather slicing optimized for HDF5 \"chunks\"\n",
    "  + hand-off between \"joined\" mapped files (see also [joining datasets](joining_datasets.ipynb))\n",
    "  + context dependent transpose\n",
    "* `MappedSource`:\n",
    "  + channel selection\n",
    "  + memory-load checks\n",
    "  \n",
    "In this section, we'll focus on the top layer. The job of `MappedSource` is fairly simple: it translates a slice for the *current* signal array geometry into a slice for the underlying data buffer. The translation is handled by `_slice_logic`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice from channel 5 to 15, from time 0 to 1000 (use the numpy.s_ object to construct slices)\n",
    "slicer = np.s_[5:15, 0:1000]\n",
    "print('Input slices:', slicer)\n",
    "buffer_slice = mapped_source._slice_logic(slicer)\n",
    "print('Buffer slices:', buffer_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two things have happened\n",
    "\n",
    "* slice(5, 15, None) has translated to a discontiguous set corresponding to `_active_channels[5:15]`\n",
    "* the buffer slice is transposed, since the buffer shape is (channels, time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other general forms of slicing are also translated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.s_[[1, 4, 10], 10:20]\n",
    "print('Input slice:', s, '---> Buffer slice:', mapped_source._slice_logic(s))\n",
    "s = np.s_[30, 10:20]\n",
    "print('Input slice:', s, '---> Buffer slice:', mapped_source._slice_logic(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mapped source also guards against loading too much memory at a time, which is governed by the \"memory_limit\" in the global params file. For demonstration, we'll temporarily over-ride this limit to a small value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Normal memory limit in bytes:', load_params().memory_limit)\n",
    "print('With 100 kB override...')\n",
    "# calcualate max samps\n",
    "max_samps = int(1e5 / mapped_source.dtype.itemsize / len(mapped_source)) + 1\n",
    "buffer_slice = mapped_source._slice_logic(np.s_[:, :max_samps])\n",
    "try:\n",
    "    # set memory limit to 100 kB\n",
    "    OVERRIDE['memory_limit'] = 1e5\n",
    "    mapped_source._check_slice_size(buffer_slice)\n",
    "except MemoryBlowOutError as e:\n",
    "    print('Got a blow out error!')\n",
    "    print('Error message', e)\n",
    "finally:\n",
    "    del OVERRIDE['memory_limit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you *really* know what you're doing, a big slice can be made using a context manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('With 100 kB override and using context...')\n",
    "try:\n",
    "    # set memory limit to 100 kB\n",
    "    OVERRIDE['memory_limit'] = 1e5\n",
    "    with mapped_source.big_slices():\n",
    "        # Any slices within this context block will not be checked for blow-out\n",
    "        mapped_source._check_slice_size(buffer_slice)\n",
    "        print('No error on big slice')\n",
    "except MemoryBlowOutError as e:\n",
    "    print('Got a blow out error!')\n",
    "    print('Error message', e)\n",
    "finally:\n",
    "    del OVERRIDE['memory_limit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory-check behavior can also be turned off for an object by setting `raise_on_big_slices=False` in the constructor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subprocess data caching\n",
    "\n",
    "Since slicing on a MappedSource might incur a significant time suck, slicing into a shared memory cache can happen in the background while the foreground process does other work. This is employed in data iteration (see [data sources notebook](data_sources.ipynb)) to allow the main process to work on the currently yielded block while the background process fills the next block's cache.\n",
    "\n",
    "This is handled in `cache_slice()` and `get_cached_slice()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time()\n",
    "# cache a big slice\n",
    "mapped_source.cache_slice(np.s_[:, :50000])\n",
    "t2 = time()\n",
    "print('Time mark 1: {}'.format(t2 - t1))\n",
    "print('Doin stuff....')\n",
    "cached_slice = mapped_source.get_cached_slice()\n",
    "t3 = time()\n",
    "print('Total cache time: {}, time available for foreground process: {}'.format(t3 - t1, t3 - t2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slicing a new MappedSource\n",
    "\n",
    "\n",
    "Generally, slicing only across channels will return all samples on those channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ch0 = mapped_source[0:2]\n",
    "print(type(data_ch0), data_ch0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it's also possible to yield a new mapped source for just those channels. This map will share a data buffer with the original map. The only difference is the `_electrode_channels` set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mapped_source.channels_are_maps():\n",
    "    new_map = mapped_source[0:2]\n",
    "print(type(new_map), new_map.shape)\n",
    "print(new_map.data_buffer is mapped_source.data_buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writeable sources and mirroring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Array-writing syntax is supported if a mapped source (and data_buffer) is writeable. The present mapped source is *not* writeable, since the file loader never maps a primary source file in read-write mode. Furthermore, since the array shape convention for data sources is (channels, time), a transposed map would *never* be writeable.\n",
    "\n",
    "To get a writeable source, this source can be mirrored. For example, the primary source would have been mirrored by the loader before applying any filtering. Mirroring has a number of options:\n",
    "\n",
    "* new_rate_ratio: the mirrored source can be prepared for downsampling if new_rate_ratio > 1\n",
    "* writeable: the new data source will be writeable (True in most use cases)\n",
    "* mapped: the new data source will be mapped -- otherwise get a `PlainArraySource`\n",
    "* channel_compatible: this controls whether the file layout will be the same as the source file, or if the number of channels will only include the active channels\n",
    "* filename: if a filename is not given, make a temporary file in the temporary file \"pool\"\n",
    "* copy: you can copy 'all' signal and aligned arrays, only 'aligned' arrays, or nothing for just empty arrays of the correct shape (copy='')\n",
    "* if any arrays are pre-allocated for the new mirror, specify them using the \"new_sources\" parameter\n",
    "\n",
    "To demonstrate a writeable map, we'll just mirror to a writeable map on a temporary file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw_source = mapped_source.mirror(writeable=True, mapped=True, copy='all')\n",
    "print('Filename:', rw_source.data_buffer.filename)\n",
    "print('Is writeable:', rw_source.writeable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two subtle things about the mirror:\n",
    "\n",
    "1. It is now a direct map, since we did not choose to keep it channel compatible. That means the active channels directly maps to the h5py.Dataset channels\n",
    "1. The data buffer is no longer transposed -- that unpleasantness was discretely tidied up by the mirror method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Is direct map:', rw_source.is_direct_map)\n",
    "print('Map shape:', rw_source.shape, '<---> Buffer shape:', rw_source.data_buffer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arbitarily blank the 1st 100 samples on all channels\n",
    "rw_source[:, :100] = 0\n",
    "print(rw_source[:5, 98:101])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data buffer: low-level array abstractions\n",
    "\n",
    "*TODO*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
