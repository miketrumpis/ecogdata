{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JobRunner for distributed computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tool enables running the same method over multiple inputs across one or more processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time, sleep\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import ecogdata.parallel.sharedmem as sm\n",
    "import ecogdata.parallel.jobrunner as jobrunner\n",
    "from ecogdata.parallel.mproc import parallel_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caveat on fork & spawn multiprocessing in notebooks\n",
    "\n",
    "**TL;DR** the ``Jobrunner`` tool is Windows-compatible, but very few of these *demonstrations* will run in Windows without moving the new class/method definitions to a separate module.\n",
    "\n",
    "In Mac and \\*nix, processes can be \"forked\" at a point in runtime, cloning memory and process state. This is very fast and compatible with notebook usage. In Windows, new processes can only \"spawn\". The process state is recovered through pickling/unpickling the parent process. This is not entirely compatible with notebook usage, since any methods/classes defined within this same script cannot be recovered. To operate in spawn mode, any of the method/class definitions below would need to be defined in a separate Python file and imported to this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Will NOT work in Windows\n",
    "    parallel_context.ctx = 'fork'\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining parallel workers\n",
    "\n",
    "The most simple `ParallelWorker` specifies a method to map arguments over, and a bit of logic to take a job specification and turn it into arguments for the method call.\n",
    "\n",
    "Here is an example using the numpy variance method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArrayVar(jobrunner.ParallelWorker):\n",
    "    para_method = staticmethod(np.var)\n",
    "    \n",
    "    def map_job(self, job):\n",
    "        \"\"\"\n",
    "        Create arguments and keywords to call self.para_method(*args, **kwargs)\n",
    "        \n",
    "        \"job\" is of the form (i, job_spec) where i is a place keeper.\n",
    "        \n",
    "        \"\"\"\n",
    "        from ecogdata.parallel.mproc import parallel_context\n",
    "        i, arr = job\n",
    "        # Do some helpful logging\n",
    "        info = parallel_context.get_logger().info\n",
    "        info('Got job {}'.format(i))\n",
    "        return i, arr, dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the salient ingredients\n",
    "\n",
    "* `para_method` is decorated as a `staticmethod` (**required**)\n",
    "* `map_job` is overloaded to construct calling arguments for `para_method`\n",
    "* the place-keeper variable `i` is kept alongside the calling arguments\n",
    "\n",
    "This overload of ``map_job`` doesn't actually change the default behavior, it is just for demonstration.\n",
    "\n",
    "Decorating the method as a `staticmethod` is required to avoid \"binding\" the method to the worker object. The use of a place-keeper index is necessary because the worker processes are asynchronous and may return values out of order. \n",
    "\n",
    "Now create a `JobRunner` to run these workers. **(This is an example of notebook code that will hang using \"spawn.\")**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_arrays = [np.random.rand(100000) for _ in range(25)]\n",
    "jobs = jobrunner.JobRunner(ArrayVar)\n",
    "# For a lot of detail, run with loglevel='info'\n",
    "res = jobs.run_jobs(inputs=plain_arrays, loglevel='error', progress=True)\n",
    "print(res[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For **very** simple parallel dispatch that splits inputs that can pass directly to a method's call signature, you can use this construction. (You can also turn off the progress bar.) **(This is an example of notebook code that will run using \"spawn,\" since everything is imported from external sources.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = jobrunner.JobRunner(np.var)\n",
    "res = jobs.run_jobs(inputs=plain_arrays, loglevel='error', progress=False)\n",
    "print(res[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also get tricky using `partial` here. The jobbed-out method is now variance computed only on the 2nd axis (`axis=1`) and each result has 5 dims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_arrays = [np.random.rand(5, 100000) for _ in range(25)]\n",
    "jobs = jobrunner.JobRunner(partial(np.var, axis=1))\n",
    "res = jobs.run_jobs(inputs=plain_arrays, loglevel='error', progress=False)\n",
    "print(res[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only required argument is the name of the `ParallelWorker` subclass. Call `run_jobs` with the inputs to distribute. If the output is an expected numerical type, the `output_shape` and `output_dtype` can be specified. By default, the number of workers is set to `mp.cpu_count()`.\n",
    "\n",
    "General return types are supported. By default, the output of `run_jobs` is an \"object array\" with `dtype=np.object`, (but simple job returns can be turned back into numerical ndarrays). Here is a general example when returning variable sized lists. (This takes advantage of the default `map_job` behavior, which just distributes the job sequence to the method.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_list(n):\n",
    "    # Return a list of random numbers.\n",
    "    # The length is equal to the job number (n).\n",
    "    return np.random.rand(n).tolist()\n",
    "\n",
    "\n",
    "class RanListWorker(jobrunner.ParallelWorker):\n",
    "    para_method = staticmethod(rand_list)\n",
    "    \n",
    "\n",
    "jobs = jobrunner.JobRunner(RanListWorker)\n",
    "# Call with n_jobs instead of inputs\n",
    "res = jobs.run_jobs(n_jobs=10, loglevel='error', progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in res:\n",
    "    print(type(r), 'len', len(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting jobs on the fly\n",
    "\n",
    "In some cases it might be useful to submit jobs sequentially. The input batch can be assembled piecemeal using the ``submitting_jobs`` context. Currently, all jobs are run *after* leaving the context, and not at the time of submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = jobrunner.JobRunner(np.var)\n",
    "# provide the \"run_jobs\" arguments here\n",
    "with jobs.submitting_jobs(progress=False):\n",
    "    for _ in range(12):\n",
    "        n = np.random.randint(low=100, high=1000)\n",
    "        print('Adding len-{} random list'.format(n))\n",
    "        jobs.submit(np.random.rand(n))\n",
    "# jobs run now\n",
    "res = jobs.output_from_submitted\n",
    "print(res[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress bar trick for nested loops\n",
    "\n",
    "*Progress bars are breaking HTML conversion. Try running the following code in Jupyter.*\n",
    "\n",
    "If you want a progress bar inside an outer loop, then create one yourself and specify `leave=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import trange, tqdm\n",
    "\n",
    "\n",
    "def sleepy(*args):\n",
    "    sleep(0.15)\n",
    "\n",
    "\n",
    "jobs = jobrunner.JobRunner(sleepy)\n",
    "for i in trange(4, desc='Running multiple stuffs'):\n",
    "    pbar = tqdm(desc='Run {} progress'.format(i + 1), total=100, leave=False)\n",
    "    jobs.run_jobs(n_jobs=100, loglevel='error', progress=pbar)\n",
    "    del(pbar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with large arrays\n",
    "\n",
    "Distributing bigger datasets to workers is slowed down by serialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_arrays = [np.random.rand(10000000) for _ in range(15)]\n",
    "print('Size in MB:', plain_arrays[0].size * plain_arrays[0].dtype.itemsize / 1024 / 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time()\n",
    "jobs = jobrunner.JobRunner(ArrayVar)\n",
    "res = jobs.run_jobs(inputs=plain_arrays, loglevel='error', progress=True)\n",
    "toc = time()\n",
    "print(res[:5], '({:.2f} seconds)'.format(toc - tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, shared memory can be accessed by each process. The `SharedmemManager` holds a `sharedctypes` array that can present itself as a numpy ndarray. Make a shared copy of the previous arrays (this incurs a bit of overhead). Skip the acquisition lock, since each array will be accessed in only one processs.\n",
    "\n",
    "**NOTE:** it is a good idea to use reference shared memory objects from the sharedmem module (e.g `sm.shared_copy`), because these lookups will change based on the parallel context. Alternatively, the objects change with context (e.g. ``parallel_context.shared_copy``)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time()\n",
    "shm_arrays = [sm.SharedmemManager(sm.shared_copy(a), use_lock=False) for a in plain_arrays]\n",
    "toc = time()\n",
    "print('Array creation: {:.2f} seconds'.format(toc - tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shared memory cannot be distributed in the same way as other objects, but they can be passed to the workers at instatiation (prior to running jobs). \n",
    "\n",
    "This worker modifies the `ArrayVar` class. It has a pointer to the list of all memory managers. The job mapping takes the job number to index that list and convert it to a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "class SharedarrayVar(ArrayVar):\n",
    "    \n",
    "    def __init__(self, shm_managers):\n",
    "        self.shm_managers = shm_managers\n",
    "    \n",
    "    def map_job(self, job):\n",
    "        # job is only the job number\n",
    "        i = job\n",
    "        info = parallel_context.get_logger().info\n",
    "        info('Got job {}'.format(i))\n",
    "        # use the get_ndarray() context manager to simply get the array\n",
    "        with self.shm_managers[i].get_ndarray() as arr:\n",
    "            pass\n",
    "        return i, (arr,), dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the `JobRunner` needs arguments to instantiate the worker: use `w_args=(shm_arrays,)`. Call `run_jobs` with the number of jobs, rather than inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time()\n",
    "jobs2 = jobrunner.JobRunner(SharedarrayVar, w_args=(shm_arrays,))\n",
    "res2 = jobs2.run_jobs(n_jobs=len(shm_arrays), loglevel='error', progress=True)\n",
    "toc = time()\n",
    "print(res2[:5], '({:.2f} seconds)'.format(toc - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 == res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real example: grid searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from ecoglib.vis.plot_util import filled_interval\n",
    "from contextlib import ExitStack\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "np.random.seed(4113)\n",
    "\n",
    "\n",
    "def new_lda_clf(n_components=1):\n",
    "    classifier = Pipeline([('scale', StandardScaler()),\n",
    "                           ('compress', vPCA(n_components=n_components)),\n",
    "                           ('lda', LinearDiscriminantAnalysis())])\n",
    "    return classifier\n",
    "\n",
    "\n",
    "def fit_predict_classifier(X_train, y_train, X_test, y_test,\n",
    "                           classifier, return_classifier=False):\n",
    "    \"\"\"Fits a classifier and predicts on holdout data. Returns signed error.\n",
    "    \"\"\"\n",
    "    clf = classifier.fit(X_train, y_train)\n",
    "    p = clf.predict(X_test)\n",
    "    if return_classifier:\n",
    "        return p - y_test, clf\n",
    "    return p - y_test\n",
    "\n",
    "\n",
    "\n",
    "class PCAGridSearch(jobrunner.ParallelWorker):\n",
    "    \"\"\"Runs single classifier fit-predict cycle on given PCA components.\n",
    "    \"\"\"\n",
    "    para_method = staticmethod(fit_predict_classifier)\n",
    "    \n",
    "    def __init__(self, cross_val_split):\n",
    "        super().__init__()\n",
    "        self.cross_val_split = [sm.SharedmemManager(x) for x in cross_val_split]\n",
    "        \n",
    "    def map_job(self, job):\n",
    "        # second part of jobs is formatted as an *args tuple: in this case a single item\n",
    "        i, comps = job\n",
    "        comps = comps[0]\n",
    "        with ExitStack() as stack:\n",
    "            arrays = [stack.enter_context(x.get_ndarray()) for x in self.cross_val_split]\n",
    "        # form the call arguments for the parallel method\n",
    "        classifier = new_lda_clf(n_components=comps)\n",
    "        args = tuple(arrays) + (classifier,)\n",
    "        return i, args, {}\n",
    "    \n",
    "    \n",
    "# Make some fake \"ecog\" trial data from 20 channels, 50 time points (5 conditions and 20 repeats).\n",
    "# Conditions are encoded by offsets\n",
    "\n",
    "trial_labels = np.repeat(np.arange(5), 20)\n",
    "channels, samples = 20, 50\n",
    "responses = np.random.randn(len(trial_labels), channels * samples) * 10\n",
    "# bias encoding\n",
    "responses += trial_labels[:, None]\n",
    "\n",
    "# Shuffle & split train/test samples 8 times.\n",
    "# For each split, grid search through the number of PCA components with a JobRunner.\n",
    "cv_gen = StratifiedShuffleSplit(n_splits=8, test_size=0.1)\n",
    "errors = []\n",
    "n_comps = np.arange(5, int(len(responses) * 0.9), 5)\n",
    "for trn, tst in tqdm(cv_gen.split(responses, trial_labels), desc='CV iter', total=8):\n",
    "    cv_split = (responses[trn], trial_labels[trn], responses[tst], trial_labels[tst])\n",
    "    runner = jobrunner.JobRunner(PCAGridSearch, w_args=(cv_split,))\n",
    "    errors.append(runner.run_jobs(n_comps, progress=False))\n",
    "    \n",
    "# Visualize results\n",
    "errors = np.array(errors)\n",
    "cls_acc = np.mean(errors == 0, axis=2)\n",
    "cls_err = np.mean(np.abs(errors), axis=2)\n",
    "\n",
    "f, axs = plt.subplots(2, 1, sharex=True)\n",
    "filled_interval(axs[0].plot, n_comps, cls_acc.mean(0), cls_acc.std(0), ax=axs[0])\n",
    "filled_interval(axs[1].plot, n_comps, cls_err.mean(0), cls_err.std(0), ax=axs[1])\n",
    "axs[0].set_ylabel('Accuracy')\n",
    "axs[1].set_ylabel('Error')\n",
    "axs[1].set_xlabel('Components')\n",
    "_ = axs[0].set_title('n-channel PCA grid search')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling errors\n",
    "\n",
    "Unexpected errors can pop up in the workers. Here's a cranky function to demonstrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hates_eights(n):\n",
    "    if n == 8:\n",
    "        raise ValueError(\"n == 8, what did you think would happen?!\")\n",
    "    return n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, it's good not to let a problem blow up the whole job. To do this, catch and supress exceptions in the JobRunner. You can still see whether there was a problem by logging errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = jobrunner.JobRunner(hates_eights)\n",
    "results = jobs.run_jobs(np.arange(10), reraise_exceptions=False, loglevel='error', progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The return value when there is an error is Not-A-Number (nan)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better idea is to get the results AND the exceptions, and re-raise exceptions after the fact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = jobrunner.JobRunner(hates_eights)\n",
    "results, exceptions = jobs.run_jobs(np.arange(10), reraise_exceptions=False, \n",
    "                                    return_exceptions=True, loglevel='error', progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All exceptions are returned as (exception-type, exception-instance, traceback)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exceptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To re-raise:\n",
    "\n",
    "```python\n",
    "raise e[1].with_traceback(e[2])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging worker errors\n",
    "\n",
    "The catch-and-suppress techniques do not allow going into the call stack with a debugger. Instead, you can run in single-threaded mode and reraise exceptions immediately. If an error is thrown in this case, you can do a debugger post-mortem to get into the stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "jobs = jobrunner.JobRunner(hates_eights, n_workers=1, single_job_in_thread=False)\n",
    "results = jobs.run_jobs(np.arange(10), reraise_exceptions=True, loglevel='error', progress=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
