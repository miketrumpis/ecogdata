{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data acquisition source files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of loading recordings from source data files (.tdms, .continuous, ...) is mostly abstracted by a new class called `ecogdata.devices.load.file2data.FileLoader`. Here is the full constructor signature. Most arguments are adapted from the previous ad-hoc loading methods defined on each system. Explanation follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Init signature:\n",
    "FileLoader(\n",
    "    experiment_path,\n",
    "    recording,\n",
    "    electrode,\n",
    "    bandpass=None,\n",
    "    notches=None,\n",
    "    units='uV',\n",
    "    load_channels=None,\n",
    "    trigger_idx=(),\n",
    "    mapped=False,\n",
    "    resample_rate=None,\n",
    "    use_stored=True,\n",
    "    save_downsamp=True,\n",
    "    store_path=None,\n",
    "    raise_on_glitch=False,\n",
    ")\n",
    "Docstring:      <no docstring>\n",
    "Init docstring:\n",
    "Data file mapping/loading. Supports downsampling.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "experiment_path: str\n",
    "    File system path where recordings are found\n",
    "recording: str\n",
    "    Name of recording in path\n",
    "electrode: str\n",
    "    Identifier of the channel map from `ecogdata.devices.electrode_pinouts`\n",
    "bandpass: sequence\n",
    "    Bandpass edges (lo, hi) -- use -1 in either place for one-sided bands\n",
    "notches: sequence\n",
    "    Sequence of line frequencies to notch\n",
    "load_channels: sequence\n",
    "    If only a subset of channels should be loaded, list them here. For example, to load channels from only one\n",
    "    port, use `load_channels=range(128)`. Otherwise all channels are used.\n",
    "trigger_idx: int or sequence\n",
    "    The index/indices of a logic-level trigger signal in this class's trigger_array\n",
    "mapped: bool or str\n",
    "    If True, leave the dataset mapped to file. Otherwise load to memory. If the (mode == 'r+') then the\n",
    "    mapped source will be writeable. (This will make a copy of the primary or downsampled datasource.)\n",
    "resample_rate: float\n",
    "    Downsample recording to this rate (must evenly divide the raw sample rate)\n",
    "use_stored: bool\n",
    "    If True, look for a pre-computed downsampled dataset\n",
    "save_downsamp: bool\n",
    "    If True, save a new downsampled dataset\n",
    "store_path: str\n",
    "    Save/load downsampled datasets at this path, rather than `experiment_path`\n",
    "raise_on_glitch: bool\n",
    "    If True, raise exceptions on unexpected events. Otherwise try to proceed with warnings.\n",
    "File:           ~/work/ecogdata/ecogdata/devices/load/file2data.py\n",
    "Type:           type\n",
    "Subclasses:     OpenEphysLoader, ActiveLoader, RHDLoader\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, these class level attributes define more system-specific rules:\n",
    "\n",
    "```python\n",
    "    # multiplier to scale raw data units to micro-volts\n",
    "    scale_to_uv = 1.0\n",
    "    # name (key) of the dataset in the HDF5 file\n",
    "    data_array = 'data'\n",
    "    # name of the dataset where a trigger signal may be found\n",
    "    trigger_array = 'data'\n",
    "    # name(s) of other auxiliary to keep track of\n",
    "    aligned_arrays = ()\n",
    "    # transpose_array is True if the data matrix is Time x Channels\n",
    "    transpose_array = False\n",
    "    # allowed file extensions\n",
    "    permissible_types = ['.h5', '.hdf']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study: Intan .rhd format\n",
    "\n",
    "Intan RHD files recorded with 2-byte integer resolution in a block structured files. They are typically recorded in 3 minute segments per file, resulting in multiple files. Due to these hurdles, RHD files are not loaded directly, but require a preprocessing step that packs multiple files into HDF5 format. That is not covered here (see `convert_rhd.py` from rhd-to-hdf5).\n",
    "\n",
    "Once packed, Intan format loading is the more simple available case study, handled almost entirely by the abstract rules in `FileLoader`. This is the entire body of the method `ecogdata.devices.load.intan.load_rhd`:\n",
    "\n",
    "```python\n",
    "loader = RHDLoader(experiment_path, test, electrode,\n",
    "                   bandpass=bandpass,\n",
    "                   notches=notches,\n",
    "                   units=units,\n",
    "                   load_channels=load_channels,\n",
    "                   trigger_idx=trigger_idx,\n",
    "                   mapped=mapped,\n",
    "                   resample_rate=useFs,\n",
    "                   use_stored=use_stored,\n",
    "                   save_downsamp=save_downsamp,\n",
    "                   store_path=store_path,\n",
    "                   raise_on_glitch=raise_on_glitches)\n",
    "return loader.create_dataset()\n",
    "```\n",
    "\n",
    "* Step 1: create a `FileLoader` subtype (`RHDLoader`) with given arguments.\n",
    "* Step 2: call the `create_dataset` method on the loader\n",
    "\n",
    "The subclass `RHDLoader` is itself fairly simple. It defines some attributes and modifies logic from the parent class to track the sampling rate definition in the original header info (preserved in a JSON string in the `h5py.File.attrs` object). In this and other casees, the `create_dataset` method is completely generalized from the basic `FileLoader` class.\n",
    "\n",
    "```python\n",
    "class RHDLoader(FileLoader):\n",
    "    scale_to_uv = 0.195\n",
    "    data_array = 'amplifier_data'\n",
    "    trigger_array = 'board_adc_data'\n",
    "    aligned_arrays = ('board_adc_data',)\n",
    "    transpose_array = False\n",
    "\n",
    "    def raw_sample_rate(self):\n",
    "        \"\"\"\n",
    "        Return full sampling rate (or -1 if there is no raw data file)\n",
    "        \"\"\"\n",
    "\n",
    "        if os.path.exists(self.primary_data_file):\n",
    "            with h5py.File(self.primary_data_file, 'r') as h5file:\n",
    "                header = json.loads(h5file.attrs['JSON_header'])\n",
    "                samp_rate = header['sample_rate']\n",
    "        else:\n",
    "            samp_rate = -1\n",
    "        return samp_rate\n",
    "\n",
    "    def create_downsample_file(self, data_file, resample_rate, downsamp_file):\n",
    "        new_file = super(RHDLoader, self).create_downsample_file(data_file, resample_rate, downsamp_file)\n",
    "        with h5py.File(data_file, 'r') as h5file:\n",
    "            header = json.loads(h5file.attrs['JSON_header'])\n",
    "        with h5py.File(new_file, 'r+') as h5file:\n",
    "            print('Putting header and closing file')\n",
    "            header['sample_rate'] = resample_rate\n",
    "            h5file.attrs['JSON_header'] = json.dumps(header)\n",
    "        return new_file\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing a `FileLoader`\n",
    "\n",
    "Create a RHDLoader to walk through the steps. Use `mapped=True` to indicate that the new dataset will have a `MappedSource` for the signal array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ecogdata.devices.load.intan import RHDLoader\n",
    "from ecogdata.expconfig import load_params\n",
    "import os\n",
    "# This is for gabilan\n",
    "# exp_path = os.path.join(\n",
    "#     os.path.join(os.path.join(load_params().network_exp, '..'), 'Human_uECoG'),\n",
    "#     'Surgery_2019_07_16'\n",
    "# )\n",
    "exp_path = os.path.join(\n",
    "    os.path.join(load_params().local_exp, 'Human_uECoG'),\n",
    "    'Surgery_2019_07_16'\n",
    ")\n",
    "\n",
    "rec_name = 'Surgery_2019-07-16_short'\n",
    "electrode = 'psv_256_rhd'\n",
    "loader = RHDLoader(exp_path, rec_name, electrode, mapped=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step in constructing a `FileLoader` is to determine data paths for source files and potential downsampling files. In this case, no downsampling is required (that comes next). The method `find_source_files` determines \n",
    "\n",
    "1. the current primary source\n",
    "1. the destination file for downsampling (a value of None here)\n",
    "1. the units scale for the primary source (default output units are microvolts)\n",
    "\n",
    "If no primary data is found, then a `DataPathError` is raised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader.find_source_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative 1: downsampling with an existing downsampled source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading with downsampling enabled engages a few more argument:\n",
    "```python\n",
    "resample_rate: float\n",
    "    Downsample recording to this rate (must evenly divide the raw sample rate)\n",
    "use_stored: bool\n",
    "    If True, look for a pre-computed downsampled dataset\n",
    "store_path: str\n",
    "    Save/load downsampled datasets at this path, rather than `experiment_path`\n",
    "```\n",
    "\n",
    "By default, `use_stored` is True which allows the use of pre-computed downsample sources. `store_path` can be specified if these pre-computed sources would be found somewhere other than the exp_path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = RHDLoader(exp_path, rec_name, electrode, resample_rate=2000)\n",
    "loader.find_source_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A resample rate of 2000 S/s is requested. Since there happens to be a pre-computed downsample file, that file shows up as a primary source. \n",
    "\n",
    "The downsample source must be a HDF5 file that is **\"channel-compatible\"** with the original file. E.g., for a recording from a single 64-channel headstage and a 61-site rat electrode, 3 data channels are present without electrode data. **These three channels must also be present in the HDF5 array.** Any arrays that are \"aligned\" with the dataset (i.e. `board_adc_data` here) also must be present in the downsample source. \n",
    "\n",
    "In `FileLoader`, these rules are satisfied by a combination of \"mirroring\" the primary data source to an empty reduced sampling rate source, and then changing the sampling from the primary source to the new source. Roughly:\n",
    "\n",
    "```python\n",
    "downsamp = datasource.mirror(new_rate_ratio=downsamp_ratio, mapped=True, channel_compatible=True,\n",
    "                             filename=downsamp_file)\n",
    "datasource.batch_change_rate(downsamp_ratio, downsamp)\n",
    "```\n",
    "\n",
    "*The _FsNNNN suffix is standard convention throughout the `FileLoader` class.* Another convention is that downsample sources have floating point arrays (single or double precision) and values are in micro-volt units. Hence the final output is now 1 instead of 0.195 for the original integer sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative 2: downsampling without an existing downsampled source\n",
    "If no downsample source exists, then another argument is engaged: whether to save the new downsample source or not. Source saving will respect `store_path`, which is the same as `exp_path` by defualt.\n",
    "```python\n",
    "save_downsamp: bool\n",
    "    If True, save a new downsampled dataset\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = RHDLoader(exp_path, rec_name, electrode, resample_rate=5000)\n",
    "loader.find_source_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, there is no precomputed source for 5000 S/s. The primary source is the full sample-rate, integer resolution file. The second value returned is now the name of the destination file for downsampling. The last value is the original ADC-to-microvolts scale. With this scenario, `FileLoader.create_downsample` (or the overloaded method) will create an appropriate downsample source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading steps in detail\n",
    "\n",
    "Basic rundown\n",
    "\n",
    "1. Establish which data channels belong to what kind of source stream: electrode, ground, and reference channels\n",
    "1. Create new downsample source if needed (into a file if appropriate) --> this becomes the new primary source\n",
    "  \n",
    "  1. File creation happens if either `save_downsamp` is True, or if the final data source will be mapped.\n",
    "\n",
    "1. Map the prevailing primary source file\n",
    "\n",
    "  1. `MappedSource` types are created whether or not the final source is mapped, *unless*\n",
    "  1. Another possibility is that downsampling is needed, but no downsample source file is needed. In that case `downsample_and_load` is called.\n",
    "  \n",
    "1. At this point data is a `MappedSource` (or `PlainArraySource` if `downsample_and_load` was used) at the correct sampling rate\n",
    "\n",
    "  1. If the final source is to be loaded and is now mapped, then create loaded mirrors.\n",
    "  1. If the data source is mapped but not writeable, then writeable mirrors are created if necessary\n",
    "  \n",
    "1. Bandpass filtering\n",
    "1. Notch filtering\n",
    "1. Event \"triggers\" are found and processed\n",
    "\n",
    "A `ecogdata.utils.Bunch` object is returned with various attributes including:\n",
    "\n",
    "* `.data`: an `ElectrodeSource` (either `MappedSource` or `PlainArraySource`)\n",
    "* `.Fs`: sampling rate\n",
    "* `.chan_map`: a `ecogdata.channel_map.ChannelMap` object\n",
    "* `.units`: the data signal units scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: channel mapping\n",
    "\n",
    "This step analyzes the channel mapping information keyed by the `electrode` parameter to figure out which data channels belong to ecog electrodes, grounded input, and reference electrodes. For the 256 channel electrode, everything goes to electrodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_map, electrode_chans, ground_chans, ref_chans = loader.make_channel_map()\n",
    "print(list(map(type, loader.make_channel_map())))\n",
    "print(list(map(len, loader.make_channel_map())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: downsampling to file\n",
    "\n",
    "This step involves calling `FileLoader.create_downsample_file`, which is fairly generalized. The `RHDLoader` first calls its super-class method, and then follows up by inserting the correct sampling rate information into the downsample source file. As mentioned before, the meat of this method is mirroring and then changing rate:\n",
    "```python\n",
    "downsamp = datasource.mirror(new_rate_ratio=downsamp_ratio, mapped=True, channel_compatible=True,\n",
    "                             filename=downsamp_file)\n",
    "datasource.batch_change_rate(downsamp_ratio, downsamp)\n",
    "```\n",
    "\n",
    "#### Step 3-4: Mapping & loading\n",
    "\n",
    "Normally a primary source is mapped as a `MappedSource` as a precursor to loading, since the data channels get organized correct and the `MappedSource.mirror(mapped=False)` has nice abstractions for handling HDF5 data. An exception is downsampling without saving, which will skip the costly  creation of a downsample file and load directly to memory. Writeable mapped mirrors are created (`MappedSource.mirror` with appropriate args) if filtering is needed, or if the loader was created with `mapped='r+'` to indicate \"writeable no matter what\".\n",
    "\n",
    "#### Step 5-6: filtering\n",
    "\n",
    "Accomplished through `datasource.filter_array` and `datasource.notch_filter`, which are defined on any `ElectrodeSource`.\n",
    "\n",
    "Step 7: trigger parsing\n",
    "\n",
    "The basic scenario is that the row at index `FileLoader.trigger_idx` in `FileLoader.trigger_array` contains a rising-edge coded event signal, which will be detected in `FileLoader.find_trigger_signals`. More complex cases (e.g. pseudo demuxed BNC channels from the National Instruments DAQ) require over-loading logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
